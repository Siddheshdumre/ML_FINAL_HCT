# -*- coding: utf-8 -*-
"""ML_FINAL_MINPROJ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oRXtGr4WU6zY1Z_Ht8-G_4GXo5RnjrD8
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

df = pd.read_csv("new_df.csv")
df

X = df.drop(columns=["efs"])
y = df["efs"]

import matplotlib.pyplot as plt
import seaborn as sns

# Compute correlation with the target
corr = df.corr(numeric_only=True)["efs"].drop("efs").sort_values()

# Split top negative and positive correlations
top_negative = corr.head(10)
top_positive = corr.tail(10)

# Combine for single plot
combined = pd.concat([top_negative, top_positive])

# Plotting
plt.figure(figsize=(12, 6))
sns.barplot(x=combined.values, y=combined.index, palette=["red" if v < 0 else "green" for v in combined.values])
plt.title("Top 10 Negatively and Positively Correlated Features with efs")
plt.xlabel("Correlation with efs")
plt.axvline(0, color='black', linewidth=0.8)
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numeric data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# Tune XGBoost
xgb = XGBRegressor(random_state=42)
xgb_params = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5, 7],
    "learning_rate": [0.01, 0.1, 0.2]
}
xgb_grid = GridSearchCV(xgb, xgb_params, scoring="r2", cv=5, verbose=1)
xgb_grid.fit(X_train, y_train)

# Tune LightGBM
lgbm = LGBMRegressor(random_state=42)
lgbm_params = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5, -1],
    "learning_rate": [0.01, 0.1, 0.2]
}
lgbm_grid = GridSearchCV(lgbm, lgbm_params, scoring="r2", cv=5, verbose=1)
lgbm_grid.fit(X_train, y_train)

from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

def evaluate_model(model, X_test, y_test, name="Model"):
    preds = model.predict(X_test)
    r2 = r2_score(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    print(f"\n{name} Performance:")
    print(f"R² Score: {r2:.4f}")
    print(f"RMSE: {rmse:.4f}")

evaluate_model(xgb_grid.best_estimator_, X_test, y_test, "Tuned XGBoost")
evaluate_model(lgbm_grid.best_estimator_, X_test, y_test, "Tuned LightGBM")

X = df.drop("efs", axis=1)
y = df["efs"]

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# Select top 30 features
selector = SelectKBest(score_func=f_regression, k=30)
X_train_sel = selector.fit_transform(X_train, y_train)
X_test_sel = selector.transform(X_test)

# Define base models
estimators = [
    ('xgb', XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)),
    ('lgbm', LGBMRegressor(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42))
]

# Stacked model
stacked_model = StackingRegressor(
    estimators=estimators,
    final_estimator=Ridge(alpha=1.0)
)

# Train
stacked_model.fit(X_train_sel, y_train)

# Evaluate
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

preds = stacked_model.predict(X_test_sel)
r2 = r2_score(y_test, preds)
rmse = np.sqrt(mean_squared_error(y_test, preds))

print(f"Stacked Model R²: {r2:.4f}")
print(f"Stacked Model RMSE: {rmse:.4f}")



!pip install optuna

import optuna
from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error
import numpy as np

# Objective function for Optuna
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)
    }

    model = LGBMRegressor(**params)
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    rmse = -cross_val_score(model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kf).mean()
    return rmse

# Run Optuna study
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=50)  # You can increase to 100+ for more accuracy

# Best parameters and result
print("Best RMSE:", study.best_value)
print("Best Parameters:", study.best_params)

best_params = study.best_params
optuna_model = LGBMRegressor(**best_params)
optuna_model.fit(X_train, y_train)

y_pred = optuna_model.predict(X_test)

r2_opt = r2_score(y_test, y_pred)
rmse_opt = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Optuna Tuned LightGBM R²: {r2_opt:.4f}")
print(f"Optuna Tuned LightGBM RMSE: {rmse_opt:.4f}")

import joblib

# Save the model
joblib.dump(optuna_model, "optuna_lgbm_model.pkl")
print("✅ Model saved as 'optuna_lgbm_model.pkl'")